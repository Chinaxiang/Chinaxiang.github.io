---
layout: post
title:  sealos å¿«é€Ÿæ­å»ºç”Ÿäº§çº§ k8s æ•™ç¨‹
date:   2024-08-22 08:28:26 +0800
tags: sealos k8s
---

![](https://bytesops.oss-cn-hangzhou.aliyuncs.com/picgo/6758dec0-11ba-4030-8696-f8e0c3c75377.png)

ä»Šå¤©åˆ†äº«ä¸€ä¸ªè¶…ç®€å•å¿«æ·ä¸”çµæ´»çš„å®‰è£… kubernetes çš„æ–¹å¼ï¼Œç§æœ‰åŒ–å¿…å¤‡ã€‚

Sealos å°±å…ˆä¸ä»‹ç»äº†ï¼Œå¤§å®¶å…ˆçŸ¥é“å®ƒæä¾›ä¸€å¥—å¼ºå¤§çš„å·¥å…·ï¼Œä½¿å¾—ç”¨æˆ·å¯ä»¥ç®¡ç†å®‰è£… Kubernetes å°±å¯ä»¥äº†ï¼Œåç»­æœ‰ç©ºå†åˆ†äº«å…¶ä»–çš„ä¸œè¥¿ï¼Œä»Šå¤©çš„ç›®çš„æ˜¯æ­å»ºä¸€å¥— k8s é›†ç¾¤ã€‚

å®˜ç½‘æ–‡æ¡£ï¼š

https://sealos.run/docs/self-hosting/lifecycle-management/

ä½¿ç”¨ Sealosï¼Œå¯ä»¥å®‰è£…ä¸€ä¸ªä¸åŒ…å«ä»»ä½•ç»„ä»¶çš„è£¸ Kubernetes é›†ç¾¤ã€‚æ­¤å¤–ï¼ŒSealos è¿˜å¯ä»¥åœ¨ Kubernetes ä¹‹ä¸Šï¼Œé€šè¿‡é›†ç¾¤é•œåƒèƒ½åŠ›ç»„è£…å„ç§ä¸Šå±‚åˆ†å¸ƒå¼åº”ç”¨ï¼Œå¦‚æ•°æ®åº“ã€æ¶ˆæ¯é˜Ÿåˆ—ç­‰ã€‚

Sealos ä¸ä»…å¯ä»¥å®‰è£…ä¸€ä¸ªå•èŠ‚ç‚¹çš„ Kubernetes å¼€å‘ç¯å¢ƒï¼Œè¿˜èƒ½æ„å»ºæ•°åƒèŠ‚ç‚¹çš„ç”Ÿäº§é«˜å¯ç”¨é›†ç¾¤ã€‚

Sealos å…·æœ‰è‡ªç”±ä¼¸ç¼©é›†ç¾¤ã€å¤‡ä»½æ¢å¤ã€é‡Šæ”¾é›†ç¾¤ç­‰åŠŸèƒ½ï¼Œå³ä½¿åœ¨ç¦»çº¿ç¯å¢ƒä¸­ï¼ŒSealos ä¹Ÿèƒ½æä¾›å‡ºè‰²çš„ Kubernetes è¿è¡Œä½“éªŒã€‚

å¥½äº†ï¼ŒåºŸè¯ä¸å¤šè¯´ï¼Œç›´æ¥å¼€æ•´ã€‚

æœ¬æœŸç›®æ ‡ï¼šå…ˆæ­å»ºå•èŠ‚ç‚¹çš„ k8s ç¯å¢ƒï¼Œç„¶åå†å‘é›†ç¾¤ä¸­æ·»åŠ èŠ‚ç‚¹ã€‚

## ä¸»æœºå‡†å¤‡

ä¸ºäº†çœåŠ²ï¼Œç”¨ money æ¢æ—¶é—´ï¼Œç›´æ¥äº‘å¹³å°å¼€ 3 å°æŠ¢å å¼ å®ä¾‹ï¼Œå¤§æ¦‚æ¯å°æ—¶ 0.7 å…ƒï¼Œé¢„è®¡ä¹Ÿå°±ä½¿ç”¨ 1-2 ä¸ªå°æ—¶å§ï¼Œå¯ä»¥æ¥å—ï¼Œä»æ™šé¥­é‡Œæ‰£ã€‚

å…³äºäº‘ä¸»æœºçš„ä½¿ç”¨å¯ä»¥å‚ç…§æˆ‘çš„å¦ä¸€ç¯‡æ–‡ç« ï¼šã€æ–‡ç« ã€‘

è¿™é‡Œç›´æ¥å¼€å¥½äº†ã€‚


![](https://bytesops.oss-cn-hangzhou.aliyuncs.com/picgo/8b9f639c-5b45-47a8-96c7-b25c048a3967.png)


## å®‰è£…å‘½ä»¤è¡Œå·¥å…·

è·å–å½“å‰æœ€æ–°çš„ç‰ˆæœ¬ï¼ˆå†™æ–‡ç« æ—¶è¿™é‡Œå½“å‰ v5.0.0ï¼‰

```
VERSION=`curl -s https://api.github.com/repos/labring/sealos/releases/latest | grep -oE '"tag_name": "[^"]+"' | head -n1 | cut -d'"' -f4`

echo $VERSION
v5.0.0
```

è‡ªåŠ¨ä¸‹è½½å¹¶å®‰è£…ï¼ˆå…¶ä¸­çš„ ghproxy æ˜¯ä¸‹è½½ github èµ„æºçš„ä»£ç†ç½‘ç«™ï¼Œä¹Ÿå¯ä»¥å»æ‰ï¼‰

```
curl -sfL https://mirror.ghproxy.com/https://raw.githubusercontent.com/labring/sealos/main/scripts/install.sh | PROXY_PREFIX=https://mirror.ghproxy.com sh -s ${VERSION} labring/sealos

[INFO]  Using v5.0.0 as release
[INFO]  Using labring/sealos as your repo
[INFO]  Downloading tar curl https://mirror.ghproxy.com/https://github.com/labring/sealos/releases/download/v5.0.0/sealos_5.0.0_linux_arm64.tar.gz
[INFO]  Downloading sealos, waiting...
######################################################################## 100.0%
sealos
[INFO]  Installing sealos to /usr/bin/sealos
SealosVersion:
  buildDate: "2024-07-11T10:14:51Z"
  compiler: gc
  gitCommit: 02327d53e
  gitVersion: 5.0.0
  goVersion: go1.20.14
  platform: linux/arm64

# æ‰§è¡Œ version å‘½ä»¤
sealos version
SealosVersion:
  buildDate: "2024-07-11T10:14:51Z"
  compiler: gc
  gitCommit: 02327d53e
  gitVersion: 5.0.0
  goVersion: go1.20.14
  platform: linux/arm64
```

æŸ¥çœ‹ help ï¼Œå†…å®¹æŒºå¤šçš„ï¼Œå¿½ç•¥å³å¯ã€‚


![](https://bytesops.oss-cn-hangzhou.aliyuncs.com/picgo/38436a6e-9341-4193-b4e3-6cd255ef22e2.png)

## å®‰è£… k8s å•æœºç‰ˆ

ä½¿ç”¨ sealos åªéœ€è¦ä¸€æ¡å‘½ä»¤å³å¯å®‰è£… k8s ï¼Œç›¸å½“ç®€å•ï¼Œå®‰è£…ä¹‹å‰ï¼Œçœ‹çœ‹å½“å‰æœ‰å“ªäº›æœ€æ–°çš„ k8s ç‰ˆæœ¬å§ã€‚

Docker Hub ä¸Šå¯ä»¥é€šè¿‡ä»¥ä¸‹é“¾æ¥æŸ¥çœ‹ Sealos æ‰€æœ‰çš„é›†ç¾¤é•œåƒï¼š

https://hub.docker.com/u/labring

ä½¿ç”¨ Registry Explorer å¯ä»¥æŸ¥çœ‹ K8s é›†ç¾¤é•œåƒçš„æ‰€æœ‰ç‰ˆæœ¬ï¼Œç›´æ¥è¾“å…¥ registry.cn-shanghai.aliyuncs.com/labring/kubernetesï¼Œç„¶åç‚¹å‡» â€œæäº¤â€ï¼š


![](https://bytesops.oss-cn-hangzhou.aliyuncs.com/picgo/df8603c3-0f9a-4611-88cb-fc471b6efc32.png)

https://explore.ggcr.dev/?repo=registry.cn-shanghai.aliyuncs.com%2Flabring%2Fkubernetes

æˆ‘è¿™é‡Œå®‰è£… k8s v1.28.9

```
# sealos version must >= v4.1.0
sealos run registry.cn-shanghai.aliyuncs.com/labring/kubernetes:v1.28.9 registry.cn-shanghai.aliyuncs.com/labring/helm:v3.9.4 registry.cn-shanghai.aliyuncs.com/labring/cilium:v1.13.4 --single

# è¾“å‡º
Flag --single has been deprecated, it defaults to running cluster in single mode when there are no master and node
2024-08-22T16:30:53 info Start to create a new cluster: master [172.21.92.19], worker [], registry 172.21.92.19
2024-08-22T16:30:53 info Executing pipeline Check in CreateProcessor.
2024-08-22T16:30:53 info checker:hostname [172.21.92.19:22]
2024-08-22T16:30:53 info checker:timeSync [172.21.92.19:22]
2024-08-22T16:30:53 info checker:containerd [172.21.92.19:22]
2024-08-22T16:30:53 info Executing pipeline PreProcess in CreateProcessor.
Trying to pull registry.cn-shanghai.aliyuncs.com/labring/kubernetes:v1.28.9...
...
Trying to pull registry.cn-shanghai.aliyuncs.com/labring/helm:v3.9.4...
...
Trying to pull registry.cn-shanghai.aliyuncs.com/labring/cilium:v1.13.4...
...
 INFO [2024-08-22 16:33:24] >> Check port kubelet port 10249..10259, reserved port 5050..5054 inuse. Please wait... 
/usr/bin/which: no docker in (/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin)
 WARN [2024-08-22 16:33:24] >> Replace disable_apparmor = false to disable_apparmor = true 
 INFO [2024-08-22 16:33:24] >> check root,port,cri success 
2024-08-22T16:33:24 info domain sealos.hub:172.21.92.19 append success
Created symlink /etc/systemd/system/multi-user.target.wants/registry.service â†’ /etc/systemd/system/registry.service.
 INFO [2024-08-22 16:33:24] >> Health check registry! 
 INFO [2024-08-22 16:33:24] >> registry is running 
 INFO [2024-08-22 16:33:24] >> init registry success 
2024-08-22T16:33:24 info domain apiserver.cluster.local:172.21.92.19 append success
Created symlink /etc/systemd/system/multi-user.target.wants/containerd.service â†’ /etc/systemd/system/containerd.service.
 INFO [2024-08-22 16:33:26] >> Health check containerd! 
 INFO [2024-08-22 16:33:26] >> containerd is running 
 INFO [2024-08-22 16:33:26] >> init containerd success 
Created symlink /etc/systemd/system/multi-user.target.wants/image-cri-shim.service â†’ /etc/systemd/system/image-cri-shim.service.
 INFO [2024-08-22 16:33:26] >> Health check image-cri-shim! 
 INFO [2024-08-22 16:33:26] >> image-cri-shim is running 
 INFO [2024-08-22 16:33:26] >> init shim success 
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
unknown system, use default to stop firewalld
* Applying /usr/lib/sysctl.d/10-default-yama-scope.conf ...
* Applying /etc/sysctl.d/50-aliyun.conf ...
...
 INFO [2024-08-22 16:33:27] >> pull pause image sealos.hub:5000/pause:3.9 
...
Created symlink /etc/systemd/system/multi-user.target.wants/kubelet.service â†’ /etc/systemd/system/kubelet.service.
 INFO [2024-08-22 16:33:27] >> init kubelet success 
 INFO [2024-08-22 16:33:27] >> init rootfs success 
...
[config/images] Pulled registry.k8s.io/kube-apiserver:v1.28.9
[config/images] Pulled registry.k8s.io/kube-controller-manager:v1.28.9
[config/images] Pulled registry.k8s.io/kube-scheduler:v1.28.9
[config/images] Pulled registry.k8s.io/kube-proxy:v1.28.9
[config/images] Pulled registry.k8s.io/pause:3.9
[config/images] Pulled registry.k8s.io/etcd:3.5.12-0
[config/images] Pulled registry.k8s.io/coredns/coredns:v1.10.1
W0822 16:33:40.520126    3186 utils.go:69] The recommended value for "healthzBindAddress" in "KubeletConfiguration" is: 127.0.0.1; the provided value is: 0.0.0.0
[init] Using Kubernetes version: v1.28.9
[preflight] Running pre-flight checks
        [WARNING FileExisting-tc]: tc not found in system path
        [WARNING Hostname]: hostname "izt4n9wcyus2gt0aeciiglz" could not be reached
        [WARNING Hostname]: hostname "izt4n9wcyus2gt0aeciiglz": lookup izt4n9wcyus2gt0aeciiglz on 100.100.2.136:53: no such host
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
W0822 16:33:40.699807    3186 checks.go:835] detected that the sandbox image "sealos.hub:5000/pause:3.9" of the container runtime is inconsistent with that used by kubeadm. It is recommended that using "registry.k8s.io/pause:3.9" as the CRI sandbox image.
[certs] Using certificateDir folder "/etc/kubernetes/pki"
...
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Using existing kubeconfig file: "/etc/kubernetes/admin.conf"
[kubeconfig] Using existing kubeconfig file: "/etc/kubernetes/kubelet.conf"
W0822 16:33:42.135254    3186 kubeconfig.go:264] a kubeconfig file "/etc/kubernetes/controller-manager.conf" exists already but has an unexpected API Server URL: expected: https://172.21.92.19:6443, got: https://apiserver.cluster.local:6443
[kubeconfig] Using existing kubeconfig file: "/etc/kubernetes/controller-manager.conf"
W0822 16:33:42.620173    3186 kubeconfig.go:264] a kubeconfig file "/etc/kubernetes/scheduler.conf" exists already but has an unexpected API Server URL: expected: https://172.21.92.19:6443, got: https://apiserver.cluster.local:6443
[kubeconfig] Using existing kubeconfig file: "/etc/kubernetes/scheduler.conf"
[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
[apiclient] All control plane components are healthy after 7.001546 seconds
[upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
...

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of control-plane nodes by copying certificate authorities
and service account keys on each node and then running the following as root:

  kubeadm join apiserver.cluster.local:6443 --token <value withheld> \
        --discovery-token-ca-cert-hash sha256:a6f327ead790b3c731d6eb9300ec668aaa8d4088d045ab37cc8aaae0af75b837 \
        --control-plane --certificate-key <value withheld>

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join apiserver.cluster.local:6443 --token <value withheld> \
        --discovery-token-ca-cert-hash sha256:a6f327ead790b3c731d6eb9300ec668aaa8d4088d045ab37cc8aaae0af75b837 
...
â„¹ï¸  Using Cilium version 1.13.4
ğŸ”® Auto-detected cluster name: kubernetes
ğŸ”® Auto-detected datapath mode: tunnel
ğŸ”® Auto-detected kube-proxy has been installed
2024-08-22T16:33:53 info succeeded in creating a new cluster, enjoy it!
2024-08-22T16:33:53 info 
      ___           ___           ___           ___       ___           ___
     /\  \         /\  \         /\  \         /\__\     /\  \         /\  \
    /::\  \       /::\  \       /::\  \       /:/  /    /::\  \       /::\  \
   /:/\ \  \     /:/\:\  \     /:/\:\  \     /:/  /    /:/\:\  \     /:/\ \  \
  _\:\~\ \  \   /::\~\:\  \   /::\~\:\  \   /:/  /    /:/  \:\  \   _\:\~\ \  \
 /\ \:\ \ \__\ /:/\:\ \:\__\ /:/\:\ \:\__\ /:/__/    /:/__/ \:\__\ /\ \:\ \ \__\
 \:\ \:\ \/__/ \:\~\:\ \/__/ \/__\:\/:/  / \:\  \    \:\  \ /:/  / \:\ \:\ \/__/
  \:\ \:\__\    \:\ \:\__\        \::/  /   \:\  \    \:\  /:/  /   \:\ \:\__\
   \:\/:/  /     \:\ \/__/        /:/  /     \:\  \    \:\/:/  /     \:\/:/  /
    \::/  /       \:\__\         /:/  /       \:\__\    \::/  /       \::/  /
     \/__/         \/__/         \/__/         \/__/     \/__/         \/__/

                  Website: https://www.sealos.io/
                  Address: github.com/labring/sealos
                  Version: 5.0.0-02327d53e
```

è‡³æ­¤ï¼Œä¸€ä¸ªè£¸å¥”çš„å•æœºé›†ç¾¤å°±å¥½äº†ï¼Œå¯ä»¥æ‰§è¡Œ kubectl å‘½ä»¤äº†


![](https://bytesops.oss-cn-hangzhou.aliyuncs.com/picgo/5af78faf-e6c7-4a97-9ac4-01c95a0eb1a5.png)

## è¿è¡Œæµ‹è¯•å®¹å™¨

é›†ç¾¤å¥½äº†ï¼Œæˆ‘ä»¬æ¥è·‘ä¸€ä¸ª echo-server å§ï¼š`jmalloc/echo-server`

ç¼–å†™ä¸€ä¸ª app.yml æ–‡ä»¶ï¼Œé‡‡ç”¨ deploy æ–¹å¼éƒ¨ç½²

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: test
  namespace: default
spec:
  replicas: 2
  selector:
    matchLabels:
      k8s-app: test
  template:
    metadata:
      labels:
        k8s-app: test
    spec:
      terminationGracePeriodSeconds: 10
      containers:
      - name: test
        image: jmalloc/echo-server
        resources:
          limits:
            cpu: 2000m
            memory: 4000Mi
          requests:
            cpu: 100m
            memory: 150Mi
        securityContext:
          privileged: false
```

éƒ¨ç½²æœåŠ¡å¹¶éªŒè¯

```
kubectl apply -f app.yml
deployment.apps/test created

kubectl get pod -o wide
NAME                    READY   STATUS              RESTARTS   AGE   IP           NODE                      NOMINATED NODE   READINESS GATES
test-74cdc6b945-l27bz   1/1     Running             0          9s    10.0.0.133   izt4n9wcyus2gt0aeciiglz   <none>           <none>
test-74cdc6b945-x2gvw   0/1     ContainerCreating   0          9s    <none>       izt4n9wcyus2gt0aeciiglz   <none>           <none>

kubectl get pod -o wide
NAME                    READY   STATUS    RESTARTS   AGE   IP           NODE                      NOMINATED NODE   READINESS GATES
test-74cdc6b945-l27bz   1/1     Running   0          49s   10.0.0.133   izt4n9wcyus2gt0aeciiglz   <none>           <none>
test-74cdc6b945-x2gvw   1/1     Running   0          49s   10.0.0.124   izt4n9wcyus2gt0aeciiglz   <none>           <none>

# æµ‹è¯•è¯·æ±‚
curl http://10.0.0.124:8080
Request served by test-74cdc6b945-x2gvw

GET / HTTP/1.1

Host: 10.0.0.124:8080
Accept: */*
User-Agent: curl/7.61.1
```

è‡³æ­¤ï¼Œå•æœºç‰ˆé›†ç¾¤çœ‹èµ·æ¥ä½¿ç”¨æ­£å¸¸ã€‚

æ¥ä¸‹æ¥æ·»åŠ ç‚¹æ–°èŠ‚ç‚¹å§ã€‚

## æ·»åŠ æ–°èŠ‚ç‚¹

æ·»åŠ èŠ‚ç‚¹ä¹Ÿæ˜¯ä¸€æ¡å‘½ä»¤ï¼Œéå¸¸ç®€å•ï¼Œæ³¨æ„ä½¿ç”¨ -p ç›®æ ‡èŠ‚ç‚¹å¯†ç ã€‚

```
sealos add --nodes 172.21.92.20 -p your_node_password
2024-08-22T17:11:50 info start to scale this cluster
2024-08-22T17:11:50 info Executing pipeline JoinCheck in ScaleProcessor.
2024-08-22T17:11:50 info checker:hostname [172.21.92.19:22 172.21.92.20:22]
2024-08-22T17:11:50 info checker:timeSync [172.21.92.19:22 172.21.92.20:22]
...

2024-08-22T17:12:04 info succeeded in joining 172.21.92.20:22 as worker
2024-08-22T17:12:04 info start to sync lvscare static pod to node: 172.21.92.20:22 master: [172.21.92.19:6443]
172.21.92.20:22 2024-08-22T17:12:04 info generator lvscare static pod is success
2024-08-22T17:12:04 info Executing pipeline RunGuest in ScaleProcessor.
2024-08-22T17:12:04 info succeeded in scaling this cluster
2024-08-22T17:12:04 info 
      ___           ___           ___           ___       ___           ___
     /\  \         /\  \         /\  \         /\__\     /\  \         /\  \
    /::\  \       /::\  \       /::\  \       /:/  /    /::\  \       /::\  \
   /:/\ \  \     /:/\:\  \     /:/\:\  \     /:/  /    /:/\:\  \     /:/\ \  \
  _\:\~\ \  \   /::\~\:\  \   /::\~\:\  \   /:/  /    /:/  \:\  \   _\:\~\ \  \
 /\ \:\ \ \__\ /:/\:\ \:\__\ /:/\:\ \:\__\ /:/__/    /:/__/ \:\__\ /\ \:\ \ \__\
 \:\ \:\ \/__/ \:\~\:\ \/__/ \/__\:\/:/  / \:\  \    \:\  \ /:/  / \:\ \:\ \/__/
  \:\ \:\__\    \:\ \:\__\        \::/  /   \:\  \    \:\  /:/  /   \:\ \:\__\
   \:\/:/  /     \:\ \/__/        /:/  /     \:\  \    \:\/:/  /     \:\/:/  /
    \::/  /       \:\__\         /:/  /       \:\__\    \::/  /       \::/  /
     \/__/         \/__/         \/__/         \/__/     \/__/         \/__/

                  Website: https://www.sealos.io/
                  Address: github.com/labring/sealos
                  Version: 5.0.0-02327d53e
```

æ‰§è¡Œ æŸ¥çœ‹èŠ‚ç‚¹ï¼ŒæŸ¥çœ‹é›†ç¾¤èŠ‚ç‚¹çŠ¶æ€ã€‚

```
kubectl get node -o wide
NAME                      STATUS   ROLES           AGE     VERSION   INTERNAL-IP    EXTERNAL-IP   OS-IMAGE                                 KERNEL-VERSION              CONTAINER-RUNTIME
izt4n9wcyus2gt0aeciiglz   Ready    control-plane   40m     v1.28.9   172.21.92.19   <none>        Alibaba Cloud Linux 3 (Soaring Falcon)   5.10.134-16.3.al8.aarch64   containerd://1.7.16
izt4n9wcyus2gt0aeciignz   Ready    <none>          2m27s   v1.28.9   172.21.92.20   <none>        Alibaba Cloud Linux 3 (Soaring Falcon)   5.10.134-16.3.al8.aarch64   containerd://1.7.16
```

åˆ é™¤ä¸€ä¸ªåˆšæ‰éƒ¨ç½²çš„ test  Podï¼ŒéªŒè¯æ˜¯å¦ä¼šåœ¨æ–°èŠ‚ç‚¹ä¸Šåˆ›å»º

```
kubectl get pod -A -o wide
NAMESPACE     NAME                                              READY   STATUS    RESTARTS   AGE     IP             NODE                      NOMINATED NODE   READINESS GATES
default       test-74cdc6b945-l27bz                             1/1     Running   0          8m47s   10.0.0.133     izt4n9wcyus2gt0aeciiglz   <none>           <none>
default       test-74cdc6b945-x2gvw                             1/1     Running   0          8m47s   10.0.0.124     izt4n9wcyus2gt0aeciiglz   <none>           <none>

kubectl delete pod test-74cdc6b945-x2gvw
pod "test-74cdc6b945-x2gvw" deleted

kubectl get pod -A -o wide
NAMESPACE     NAME                                              READY   STATUS              RESTARTS   AGE     IP             NODE                      NOMINATED NODE   READINESS GATES
default       test-74cdc6b945-l27bz                             1/1     Running             0          9m13s   10.0.0.133     izt4n9wcyus2gt0aeciiglz   <none>           <none>
default       test-74cdc6b945-wm7rk                             0/1     ContainerCreating   0          4s      <none>         izt4n9wcyus2gt0aeciignz   <none>           <none>

```

æ­£å¸¸ï¼Œåœ¨æ–°èŠ‚ç‚¹ä¸ŠæˆåŠŸåˆ›å»ºæ–°çš„ Pod.

## åˆ é™¤èŠ‚ç‚¹

æœ‰æ—¶å€™ä¸ä»…ä»…è¦å¢åŠ  node è¿˜éœ€è¦åˆ é™¤ node, ä¸€æ ·ä¸€è¡Œå‘½ä»¤ï¼Œä¸åŒä¹‹å¤„æ˜¯éœ€è¦ç¡®è®¤ï¼Œä¸éœ€è¦æŒ‡å®š -p å¯†ç ã€‚

```
sealos delete --nodes 172.21.92.20
2024-08-22T17:22:22 info are you sure to delete these nodes?
Yes [y/yes], No [n/no]: y
2024-08-22T17:22:26 info start to scale this cluster
2024-08-22T17:22:26 info Executing pipeline DeleteCheck in ScaleProcessor.
...

kubectl get node -o wide

NAME                      STATUS   ROLES           AGE   VERSION   INTERNAL-IP    EXTERNAL-IP   OS-IMAGE                                 KERNEL-VERSION              CONTAINER-RUNTIME
izt4n9wcyus2gt0aeciiglz   Ready    control-plane   50m   v1.28.9   172.21.92.19   <none>        Alibaba Cloud Linux 3 (Soaring Falcon)   5.10.134-16.3.al8.aarch64   containerd://1.7.16


kubectl get pod -A -o wide

NAMESPACE     NAME                                              READY   STATUS    RESTARTS   AGE   IP             NODE                      NOMINATED NODE   READINESS GATES
default       test-74cdc6b945-hbq88                             1/1     Running   0          81s   10.0.0.135     izt4n9wcyus2gt0aeciiglz   <none>           <none>
default       test-74cdc6b945-l27bz                             1/1     Running   0          18m   10.0.0.133     izt4n9wcyus2gt0aeciiglz   <none>           <none>
kube-system   cilium-operator-596cc99774-hbvzp                  1/1     Running   0          50m   172.21.92.19   izt4n9wcyus2gt0aeciiglz   <none>           <none>
kube-system   cilium-x9t6r                                      1/1     Running   0          50m   172.21.92.19   izt4n9wcyus2gt0aeciiglz   <none>           <none>
kube-system   coredns-5dd5756b68-9xqlb                          1/1     Running   0          50m   10.0.0.86      izt4n9wcyus2gt0aeciiglz   <none>           <none>
kube-system   coredns-5dd5756b68-cdp5b                          1/1     Running   0          50m   10.0.0.32      izt4n9wcyus2gt0aeciiglz   <none>           <none>
kube-system   etcd-izt4n9wcyus2gt0aeciiglz                      1/1     Running   0          50m   172.21.92.19   izt4n9wcyus2gt0aeciiglz   <none>           <none>
kube-system   kube-apiserver-izt4n9wcyus2gt0aeciiglz            1/1     Running   0          50m   172.21.92.19   izt4n9wcyus2gt0aeciiglz   <none>           <none>
kube-system   kube-controller-manager-izt4n9wcyus2gt0aeciiglz   1/1     Running   0          50m   172.21.92.19   izt4n9wcyus2gt0aeciiglz   <none>           <none>
kube-system   kube-proxy-dcqzz                                  1/1     Running   0          50m   172.21.92.19   izt4n9wcyus2gt0aeciiglz   <none>           <none>
kube-system   kube-scheduler-izt4n9wcyus2gt0aeciiglz            1/1     Running   0          50m   172.21.92.19   izt4n9wcyus2gt0aeciiglz   <none>           <none>
```

## æ¸…ç† k8s é›†ç¾¤

ä¹Ÿæ¥ä½“éªŒä¸€ä¸‹æ¸…ç† K8s é›†ç¾¤å§ï¼Œéšä¾¿æŠ˜è…¾ã€‚

```
sealos reset
2024-08-22T17:27:15 info are you sure to delete these nodes?
Yes [y/yes], No [n/no]: y
2024-08-22T17:27:18 info start to delete Cluster: master [172.21.92.19], node []
2024-08-22T17:27:18 info start to reset nodes: []
2024-08-22T17:27:18 info start to reset masters: [172.21.92.19:22]
2024-08-22T17:27:18 info start to reset node: 172.21.92.19:22
...

2024-08-22T17:27:22 info succeeded in deleting current cluster
2024-08-22T17:27:22 info 
      ___           ___           ___           ___       ___           ___
     /\  \         /\  \         /\  \         /\__\     /\  \         /\  \
    /::\  \       /::\  \       /::\  \       /:/  /    /::\  \       /::\  \
   /:/\ \  \     /:/\:\  \     /:/\:\  \     /:/  /    /:/\:\  \     /:/\ \  \
  _\:\~\ \  \   /::\~\:\  \   /::\~\:\  \   /:/  /    /:/  \:\  \   _\:\~\ \  \
 /\ \:\ \ \__\ /:/\:\ \:\__\ /:/\:\ \:\__\ /:/__/    /:/__/ \:\__\ /\ \:\ \ \__\
 \:\ \:\ \/__/ \:\~\:\ \/__/ \/__\:\/:/  / \:\  \    \:\  \ /:/  / \:\ \:\ \/__/
  \:\ \:\__\    \:\ \:\__\        \::/  /   \:\  \    \:\  /:/  /   \:\ \:\__\
   \:\/:/  /     \:\ \/__/        /:/  /     \:\  \    \:\/:/  /     \:\/:/  /
    \::/  /       \:\__\         /:/  /       \:\__\    \::/  /       \::/  /
     \/__/         \/__/         \/__/         \/__/     \/__/         \/__/

                  Website: https://www.sealos.io/
                  Address: github.com/labring/sealos
                  Version: 5.0.0-02327d53e


kubectl
-bash: /usr/bin/kubectl: No such file or directory
```

## ç¦»çº¿å®‰è£…

ç¦»çº¿å®‰è£…ä¹Ÿæ¯”è¾ƒç®€å•ï¼Œsealos æœ¬èº«æ˜¯ä¸€ä¸ªäºŒè¿›åˆ¶æ–‡ä»¶ï¼Œä½ æ”¾ç½®åˆ°æœºå™¨ä¸Š /usr/bin/ ç›®å½•ä¸‹å³å¯ï¼Œsealos å®‰è£…ä¾èµ–çš„é•œåƒå¯ä»¥é€šè¿‡æœ‰ç½‘ç»œç¯å¢ƒçš„æœºå™¨è¿›è¡Œå¯¼å‡ºä¸º tar åŒ…ï¼Œå†å¯¼å…¥å³å¯ã€‚

```
sealos pull registry.cn-shanghai.aliyuncs.com/labring/kubernetes:v1.28.9
sealos save -o kubernetes.tar registry.cn-shanghai.aliyuncs.com/labring/kubernetes:v1.28.9

```


![](https://bytesops.oss-cn-hangzhou.aliyuncs.com/picgo/e5c923f2-40ad-402c-9578-a8b5919c0e25.png)


å…¶ä»–é•œåƒåŒç†ã€‚

ç„¶ååœ¨æ— å¤–ç½‘ç¯å¢ƒçš„æœºå™¨ä¸Šå¯¼å…¥å³å¯ã€‚

```
sealos load -i kubernetes.tar
sealos images
```

## æ€»ç»“

å¥½äº†ï¼Œä»Šå¤©ç»™å¤§å®¶åˆ†äº«äº†ä¸€ä¸ªè¶…ç®€å•ä¸”ä¾¿æ·çš„ k8s é›†ç¾¤ç¯å¢ƒå®‰è£…æ–¹å¼ï¼Œä½ å­¦ä¼šäº†å—ï¼Ÿæœ‰ä»»ä½•é—®é¢˜æ¬¢è¿é€šè¿‡æˆ‘çš„ä¸ªäººå…¬&å·ï¼š**æ–°è´¨ç¨‹åºçŒ¿**ï¼Œæ‰¾åˆ°æˆ‘ï¼Œä¸€èµ·å‡çº§æ‰“æ€ªï¼ˆåˆšä¹°äº† **é»‘ç¥è¯ï¼šæ‚Ÿç©º**ï¼Œå‘¨æœ«å¥½å¥½ç©ä¸€ç©ï¼‰ã€‚